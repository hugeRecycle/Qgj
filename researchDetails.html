<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" id="wixDesktopViewport" />
		<title>Projects</title>
		<link rel="shortcut icon" href="./favicon.ico">
		<link rel="stylesheet" type="text/css" href="./css/common.css" />
		<link rel="stylesheet" type="text/css" href="./css/researchDetails.css" />
		<script src="./js/jquery-2.1.4.min.js"></script>
	</head>
	<body>
		<header class="header-box">
			<div class="logo-box">
				<img src="./img/icon_headerLogo_l.png" title="西湖大学" onclick="openPage('https://www.westlake.edu.cn/', '_blank')" />
				<span class="splitline"></span>
				<img class="labLogo" src="./img/icon_headerLogo_r.png" />
            </div>
			<div class="r-box">
				<nav class="menu-box">
					<ul>
						<li><a href="index.html">Home</a></li>
						<li class="active"><a>Research<img src="./img/icon_arrowDown_active.png" /></a>
							<ul class="childList">
								<li><a href="research.html">Overview of Our Rescarch</a></li>
								<li class="childActive"><a href="foundationModels.html">Multimodal Foundation Models</a></li>
								<li><a href="algorithms.html">AIGC Theory and Algorithms</a></li>
								<li><a href="AIGCModels.html">Image, Video and 3D Content AIGC Models</a></li>
							</ul>
						</li>
						<li><a href="team.html">Team</a></li>
						<li><a>Contributions<img src="./img/icon_arrowDown.png" /></a>
							<ul class="childList" style="width: 2.01rem;transform: translateX(-17%);">
								<li><a href="journalPapers.html">Publications</a></li>
								<li><a href="awarded.html">Patents</a></li>
							</ul>
						</li>
						<li><a href="news.html">News</a></li>
						<li><a href="joinus.html">Join us</a></li>
					</ul>
				</nav>
				<div class="language-box">
					<img src="./img/icon_language.png" />
					<span>中文</span>
				</div>
			</div>
		</header>
		<section class="banner-box">
			<img src="./images/projects/banner.png" />
			<div class="banner-title">
				<div class="txt">
					<span>Projects</span>
				</div>
			</div>
		</section>
		<main class="main-box">
            <div class="pageSignature-box">
                <a href="index.html">Home</a>
                <span class="arrow">>></span>
                <a href="research.html">Research</a>
                <span class="arrow">>></span>
                <a href="foundationModels.html">Multimodal Foundation Models</a>
                <span class="arrow">>></span>
                <a>Openstory++ : A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling</a>
            </div>
			<div class="itemBlock">
				<div class="container-box">
                    <div class="item-box">
                        <h5>Openstory++ : A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling</h5>
                        <img src="./images/projects/img_10.png" />
                        <p style="text-align: center;font-size: .2rem; font-family: Avenir-Medium;font-weight: 500;">The visualization of our dataset. On the left is a data case with visual annotation that corresponds to each entity word in the sentence, where different color stands for different instance visual annotations, and on the right is the general pipeline of our dataset annotation process.</p>
                        <strong>Abstract</strong>
                        <p>Recent image generation models excel at creating high-quality images from brief captions. However, they fail to maintain consistency of multiple instances across images when encountering lengthy contexts. This inconsistency is largely due to in existing training datasets the absence of granular instance feature labeling in existing training datasets. To tackle these issues, we introduce Openstory++, a large scale dataset combining additional instance-level annotations with both images and text. This dataset can be utilized to train multi-modal generated models, allowing for the training of instance-focused story visualization models. Furthermore, we develop a tailored training methodology that emphasizes entity-centric image-text generation, ensuring that the models learn to effectively interweave visual and textual information. Specifically, Openstory++ streamlines the process of keyframe extraction from open-domain videos, employing vision-language models to generate captions that are then polished by a large language model for narrative continuity. It surpasses previous datasets by offering a more expansive open-domain resource, which incorporates automated captioning, high-resolution imagery tailored for instance count, and extensive frame sequences for temporal consistency. Additionally, we present Cohere-Bench, a pioneering benchmark framework for evaluating the image generation tasks when long multimodal context is provided, including the ability to keep the background, style, instances in the given context coherent. Compared to existing benchmarks, our work fills critical gaps in multi-modal generation, propelling the development of models that can adeptly generate and interpret complex narratives in open-domain environments. Experiments conducted within Cohere-Bench confirm the superiority of Openstory++ in nurturing high-quality visual storytelling models, enhancing their ability to address sophisticated and open-domain generation tasks.</p>
                        <strong>Method</strong>
                        <img src="./images/projects/img_11.png" />
                        <p>This figure showcases the workflow of our pipeline. After obtaining a sequence of frames devoid of redundancy, we first utilized BLIP2 to generate basic image captions. Subsequently, Video-LLaVA was employed to produce a sequence of captions that encapsulate the narrative flow. Guided by the sequence caption, a LLM was prompted to align the entities in the image captions, thus enhancing the narrative coherence across consecutive frames. Next, YOLO-World was applied to detect bounding boxes for the entities. To ensure that labels for the same entities across frames are unique and consistent, we blended the bounding box labels with the assistance of Dino and a facial feature module. Finally, we employed EfficientVIT-SAM to obtain the masks for the entities, thereby providing a comprehensive understanding of the spatial extent and characteristics of each entity within the frames.</p>
                    </div>
				</div>
			</div>
		</main>
		<footer class="footer-box">
			<img src="./img/img_footerBg.png" />
			<div class="container-box">
				<div class="joinus-box">
					<h6>JOIN US</h6>
					<p>I am hiring Research Fellows, associate Fellows, assistant Fellows, postdoctoral fellows, algorithm engineers, technical artists, research assistants, and administrative assistants. Interested candidates can reach me directly.</p>
					<a href="joinus.html">Contact Us</a>
				</div>
				
				<div class="logo-box">
					<img src="./img/icon_footerLogo_l.png" />
					<span class="splitline"></span>
					<img class="labLogo" src="./img/icon_footerLogo_r.png" />
				</div>
				<p class="address">Address: Yunchuang Gallium Valley 6-201, No. 428 Zhiqiang Road, Sandun Town, Xihu District, Hangzhou, Zhejiang Province</p>
				<p class="email">Email: maple_hr@westlake.edu.cn</p>
				
				<p class="filings">
					<span><a href="https://beian.miit.gov.cn/" target="_blank">浙ICP备18025489号</a></span>
					<span>浙公安备33010602007514号</span>
					<span>Copyright © Westlake University. All Rights Reserved</span>
				</p>
			</div>
		</footer>
		<script>
			function setRem(designSize) {
				let html = document.documentElement;
				let width = html.clientWidth;
				if (designSize == 3.75) {
					width = width >= 375 ? 375 : width;
				}
				if (designSize == 14.4) {
					width = width >= 1440 ? 1440 : width;
				}
				html.style.fontSize = width / designSize + 'px'; // 设计稿宽度为1440px，那么1rem=100px，故除以14.4
			}
			
			function openPage(url, target) {
				if (url) {
					window.open(url, target || '_self')
				}
			}
			window.onresize = function() {
				setRem(14.4);
			};
			$(function(){
				setRem(14.4);
			});
		</script>
	</body>
</html>